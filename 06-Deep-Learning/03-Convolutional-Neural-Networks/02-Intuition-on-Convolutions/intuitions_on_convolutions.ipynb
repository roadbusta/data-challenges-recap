{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuitions on Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build intuitions about these **`convolution operations`**.\n",
    "\n",
    "üéØ <b><u>Goals</u></b>\n",
    "- **Compute convolution operations**\n",
    "- **Visualize**\n",
    "    - convolution kernels\n",
    "    - the effects of a convolution kernel applied on images\n",
    "\n",
    "<hr>\n",
    "\n",
    "üñº <b><u>Convolutional Neural Networks are Neural Networks specifically designed to work on images</u></b>. \n",
    "\n",
    "This is made possible thanks to **convolutions**.\n",
    "\n",
    "These specific mathematical operations apply a **`kernel`** to an input image and creates an output representation. The name of this **`output`** can change depending on the community. Here, let's talk about the output as the **`output of a layer`**, as in standard NeuralNet models. But for Convolutional Neural Networks, it can also be called :\n",
    "* a **`\"convoluted representation/feature\"`**,\n",
    "* or a **`\"convolution\"`**,\n",
    "* or also an **`\"activation\"`** (as it corresponds to the activation of a given layer).\n",
    "\n",
    "<img src=\"convolution.png\" width=\"300\">\n",
    "\n",
    "---\n",
    "\n",
    "‚ùóÔ∏è <b><u>Remarks</u></b> ‚ùóÔ∏è\n",
    "\n",
    "* It is important to understand that **the same kernel, i.e. the same weights, are applied to different zones of the images**. \n",
    "\n",
    "* This is completely different from Dense Neural Networks that we've been working with during the last two chapters:\n",
    "    * In `Dense/\"Fully Connected\" Neural Network`, each weight of each neuron is related to only one input coordinate (which in this case would be each pixel).\n",
    "    * In a `Convolution Neural Network`, the weights of a kernel are not applied to only one feature input, i.e. one pixel, but to different pixels, \"step by step\" !\n",
    "\n",
    "üëâ You can think of each kernel (or each filter in the case of colored images) as a **`magnifying glass`** through which you see the image. Similarly to your eyes, kernels cannot capture everything in a picture at once, but they ***scan different parts of a picture before understanding the whole picture which is being analyzed***.\n",
    "\n",
    "üé¨ So let's have a closer look at `convolution operations`, and their impact in `Convolutional Neural Networks`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚ùì First use the following function `load_data` to... to load the data ‚ùì\n",
    "\n",
    "* Do not change anything in the function !\n",
    "* Restrict from any desire to change the shapes or types of the outputs! This will have an impact on further questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def load_data(path):\n",
    "    nb_circles = 100\n",
    "    nb_triangles = 100\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(nb_circles):\n",
    "        c_path = os.path.join(path, 'circles', f'circle_{i}.png')\n",
    "        X.append(imread(c_path)[:, :, :1])\n",
    "        y.append(0)\n",
    "    \n",
    "    for i in range(nb_triangles):\n",
    "        t_path = os.path.join(path, 'triangles', f'triangle_{i}.png')\n",
    "        X.append(imread(t_path)[:, :, :1])\n",
    "        y.append(1)\n",
    "        \n",
    "    c = list(zip(X, y))\n",
    "    np.random.shuffle(c)\n",
    "    X, y = zip(*c)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "        \n",
    "X, y = load_data(\"data\")\n",
    "# Replace data by \"https://wagon-public-datasets.s3.amazonaws.com/deep-learning-circles-triangles\" \n",
    "# if you are on a server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question about the shape** ‚ùì\n",
    "\n",
    "* How many images do we have ?\n",
    "* What are their dimensions ? \n",
    "* Can you comment on the number of channels ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>About the number of channels:</i></summary>\n",
    "   \n",
    "Actually, we have already talked about this during the `MNIST challenge`.        \n",
    "        \n",
    "*  We need only one channel to compute the \"*blackness intensity*\" of a pixel with 0 corresponding to a black pixel and 1 corresponding to a white pixel. The last dimension corresponds to some kind of  \"Black to white channel\". \n",
    "        \n",
    "üé® For colored images, the last dimension would be equal to 3 for `Red, Green, Blue (RGB)`\n",
    "\n",
    "üëâ Have fun playing with the intensities of Red, Green and Blue <a href=\"https://www.w3schools.com/colors/colors_rgb.asp\">`here`</a>\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question about the normalization** ‚ùì\n",
    "\n",
    "Do these images need some normalization ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** : Display some images with `plt.imshow` and their respective labels ‚ùì\n",
    "\n",
    "_Note: the images are black and white, therefore use `cmap=gray` in the dedicated matplotlib function - otherwise, you will see unrelevant and weird colors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: How many classes are we going to predict** ‚ùì\n",
    "\n",
    "_It should already give you some information about how to design  the last layer of your Convolutional Network_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function `compute_convolution` performs a **convolution operation** $ \\Leftrightarrow $ i.e. *it applies a kernel to an image*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è <b><u>Vocabulary warning</u></b> ‚ùóÔ∏è\n",
    "\n",
    "1. \"convolution\" sometimes refers to _one_ operation. It can also refer to the convolution operations repeated/slid on the entire image.\n",
    "\n",
    "\n",
    "2. For [historical reasons](https://towardsdatascience.com/convolution-vs-correlation-af868b6b4fb5), \"_convolution_\" in CNN is slightly different from \"_convolution\" in Signal Preprocessing, but both are very close\n",
    "    - use [`scipy.ndimage.correlate`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.correlate.html#scipy.ndimage.correlate) for CNN_type convolution\n",
    "    \n",
    "    - instead of standard [`scipy.ndimage.convolve`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.convolve.html#scipy.ndimage.convolve) or [`numpy.convolve`](https://numpy.org/doc/stable/reference/generated/numpy.convolve.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question about the `compute_convolution` function** ‚ùì\n",
    "\n",
    "Run it and try to understand the different steps of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_convolution(img, kernel):\n",
    "    # Parameters\n",
    "    kernel = np.array(kernel)\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "\n",
    "    img = np.squeeze(img) # Removes dimensions of size 1\n",
    "    img_height, img_width = img.shape\n",
    "    \n",
    "    array = []\n",
    "\n",
    "    for x in range(img_height - kernel_height):\n",
    "        arr = []\n",
    "        \n",
    "        for y in range(img_width - kernel_width):\n",
    "            \n",
    "            a = np.multiply(img[x: x + kernel_height, y: y + kernel_width], kernel)\n",
    "            arr.append(a.sum())\n",
    "            \n",
    "        array.append(arr)\n",
    "        \n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **How does the `compute_convolution` function work in practice** ‚ùì \n",
    "\n",
    "\n",
    "* Choose any image from the input dataset\n",
    "* Apply the following kernel to it\n",
    "* Display both the input image and the output image. \n",
    "* Do you see differences ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_kernel = [\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "        \n",
    "üßëüèª‚Äçüè´ The previous kernel corresponds to the **`identity_kernel`**, meaning that **the output is equal to the input**... \n",
    "    \n",
    "üïµüèª‚Äç‚ôÇÔ∏è It basically did nothing to the input image. It you think about it thoroughly, that's not surprising. With this kernel, only the pixel scanned in the middle is kept and multiplied by one, the rest is multiplied by zero.        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéÅ We coded a function `plot_convolution` that plots the output image from an input image on which a kernel was applied.\n",
    "\n",
    "‚ùì **Question**: `plot_convolution` with the following`kernel_1` ‚ùì\n",
    "\n",
    "Apply it once on an triangle and once on a circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convolution(img, kernel, activation=False):\n",
    "    ''' The following printing function ease the visualization'''\n",
    "    \n",
    "    img = np.squeeze(img)\n",
    "    output_img = compute_convolution(img, kernel)\n",
    "    if activation:\n",
    "        output_img = np.maximum(output_img, 0)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    ax1 = plt.subplot2grid((3,3),(0,0), rowspan=3)\n",
    "    ax1.imshow(img, cmap='gray')\n",
    "    ax1.title.set_text('Input image')\n",
    "    \n",
    "    ax2 = plt.subplot2grid((3,3),(1, 1))\n",
    "    ax2.imshow(kernel, cmap='gray')\n",
    "    ax2.title.set_text('Kernel')    \n",
    "    \n",
    "    ax3 = plt.subplot2grid((3,3),(0, 2), rowspan=3)\n",
    "    ax3.imshow(output_img, cmap='gray')\n",
    "    ax3.title.set_text('Output image')    \n",
    "\n",
    "    for ax in [ax1, ax2, ax3]:\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_1 = [\n",
    "    [1, 1, 1],\n",
    "    [0, 0, 0],\n",
    "    [-1, -1, -1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ Let's analyse what just happened:\n",
    "\n",
    "* White pixels correspond to high values and black pixels to low values.\n",
    "* In a Neural Network, in a Dense Layer or a Convolutional Layer, there is an activation function. \n",
    "    * *For example*, when the activation function is `relu`, you already know that it simply correponds to setting the negative values to 0.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "‚ùì **What is the impact of the activation function in a Convolutional Layer ? ** ‚ùì\n",
    "\n",
    "Re-run the previous function `plot_convolution` with `activation` set to `True` (in this case, the activation function _is_ the relu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ This kernel is actually highlighting the edges in a given direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Play with different kernels...** ‚ùì\n",
    "\n",
    "Try the following kernels to check the different edges it can detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_2 = [\n",
    "    [-1, -1, -1],\n",
    "    [0, 0, 0],   \n",
    "    [1, 1, 1],\n",
    "]\n",
    "\n",
    "kernel_3 = [\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1]\n",
    "]\n",
    "\n",
    "kernel_4 = [\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **What is the effect of the kernel size** ‚ùì\n",
    "\n",
    "Try the _kernel_big_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_big = np.array([\n",
    "    np.ones((10,)),\n",
    "    np.ones((10,)),\n",
    "    np.ones((10,)),\n",
    "    np.zeros((10,)),\n",
    "    np.zeros((10,)),\n",
    "    np.zeros((10,)),\n",
    "    np.zeros((10,)),\n",
    "    np.ones((10,))*-1,\n",
    "    np.ones((10,))*-1,\n",
    "    np.ones((10,))*-1,\n",
    "])\n",
    "\n",
    "kernel_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Try another kernel**  ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_kernel = np.random.uniform(-10, 10, (5, 5))\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've got the idea of what a convolution operation does to an image, let's see how it goes with a \"real\" Convolutional Neural Network. ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Architecture and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: Designing a CNN** ‚ùì\n",
    "\n",
    "Write a Convolutional Network that has \n",
    "- a Convolutional Layer with 16 filters with $ (4, 4) $ kernels.\n",
    "- a Convolutional Layer with 32 filters with $ (3, 3) $ kernels.\n",
    "- a Convolutional Layer with 32 filters with $ (3, 3) $ kernels.\n",
    "- a Convolutional Layer with 32 filters with $ (2, 2) $ kernels.\n",
    "\n",
    "with:\n",
    "- A Max-Pooling Layer (with a $ (2, 2) $ pool-size) after each convolution.\n",
    "- A Hidden Dense Layer with the size of your choice, be reasonable:\n",
    "    - after the flattening part \n",
    "    - but before the last layer\n",
    "\n",
    "\n",
    "Also, make sure to compile your model with the adequate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def initialize_model():\n",
    "    \n",
    "    pass  # YOUR CODE HERE    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Training the CNN** ‚ùì\n",
    "\n",
    "* Fit the model. You should achieve a accuracy of *at least* 90%. \n",
    "\n",
    "    * When you reach such a high score, it may sound suspicious and you would probably ask yourself whether the model is overfitting or not... but let's ignore it for this challenge üòè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéÅ We coded the `plot_loss_accuracy` for you.\n",
    "\n",
    "‚ùì **Question: does the CNN converge** ‚ùì\n",
    "\n",
    "_Also, do you see any sign of overfitting ?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history, title=None):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "    \n",
    "    # --- LOSS --- \n",
    "    \n",
    "    ax[0].plot(history.history['loss'])\n",
    "    ax[0].plot(history.history['val_loss'])\n",
    "    ax[0].set_title('Model loss')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylim((0,3))\n",
    "    ax[0].legend(['Train', 'Test'], loc='best')\n",
    "    ax[0].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[0].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    # --- ACCURACY\n",
    "    \n",
    "    ax[1].plot(history.history['accuracy'])\n",
    "    ax[1].plot(history.history['val_accuracy'])\n",
    "    ax[1].set_title('Model Accuracy')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Test'], loc='best')\n",
    "    ax[1].set_ylim((0,1))\n",
    "    ax[1].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[1].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    if title:\n",
    "        fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Deeper understanding of the CN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üïµÔ∏è‚Äç‚ôÄÔ∏è With the following table, you will have a better overview of the different weights, kernels and filters involved in the CNN you've built earlier:\n",
    "\n",
    "| layer_number | convolution_layer | kernel_number | channel_number |\n",
    "|--------------|-------------------|---------------|----------------|\n",
    "| 0            | conv2D no 1       | 16            | 1              |\n",
    "| 2            | conv2D no 2       | 32            | 16             |\n",
    "| 4            | conv2D no 3       | 64            | 32             |\n",
    "| 8            | conv2D no 4       | 64            | 64             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2.1) Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî• It is possible to **retrieve the values of all the kernels after training a CNN**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üë©üèª‚Äçüè´ For any Sequential Neural Network (Dense or Convolutional), you can:\n",
    "- print the **`.summary()`** to display the layers and the number of weights/parameters involved\n",
    "- access the differents **`.layers`** of your model\n",
    "- access the differents parameters **`.weights`** of this layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **List the different layers of your CNN model** ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Try to access the weights matrix `w` of your first convolution layer, then your second one** ‚ùì  \n",
    "hint: layer parameters consist of weight matrix and biases (w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **What do their shapes represent** ‚ùì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: visualizing the first layer** ‚ùì \n",
    "\n",
    "The original picture had 1 channel.. and we created 16 outputs using 16 kernels !\n",
    "\n",
    "ü™Ñ Using `plot_convolution(activation = True)`, display some kernels from the first convolutional layer, along with the activation output, to see what the model has learned from the images in this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2.2) Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been looking at the activation (\"`output image`\") of the **first convolutional layer** only.\n",
    "\n",
    "ü§î What if we want to **visualize the activation of an image after the second convolutional layer of the CNN, or the third one?**\n",
    "\n",
    "1. We'll need to compute activation from the first layer\n",
    "2. Then feed it to the second to compute activation of the second one\n",
    "3. Then feed it to the third ...\n",
    "4. etc...\n",
    "\n",
    "üëá We'll do it for you below using the  `tensorflow.keras` `Fonctional API` syntax! Make sure you understand it all !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><i>Step 1 : list all the 11 layers' outputs of your CNN</i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_outputs = [layer.output for layer in model.layers]\n",
    "layers_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><i>Step 2: Instantiate 11 sub-models re-using already trained weights and biases</i></u> \n",
    "- layer1\n",
    "- layer1-->layer2\n",
    "- layer1-->layer2-->layer3\n",
    "- ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_models = [Model(inputs=model.input, outputs=output) for output in layers_outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><i>Step 3: Compute the outputs of each submodel</i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [m.predict(X) for m in activation_models]\n",
    "len(activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Pro tips below! In tensorflow, you could also create a single model with many outputs to avoid python loops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_outputs = [layer.output for layer in model.layers] # same than above\n",
    "activation_model = Model(inputs=model.input, outputs=layers_outputs) # model with many outputs !\n",
    "activations = activation_model.predict(X) # 11 prediction at once!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Now that we computed activations, choose one image and display the activation \"images\" of each convolutional layer** ‚ùì \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üßëüèª‚Äçüè´ ***Notes*** üßëüèª‚Äçüè´\n",
    "\n",
    "1. Notice how the information of an image **flows** through the Convolutional Neural Network.\n",
    "2. You should see the picture becoming more and more \"abstract\", of smaller and smaller \"dimensions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulations!**\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) (Appendix) Utils\n",
    "\n",
    "* The following section simply presents the functions that helped us create the dataset with triangles and circles you have been working with.\n",
    "\n",
    "* They were left at the end of the notebook just in case you want to further prototype and get better understanding of what is going on. \n",
    "\n",
    "* But skip this section and go to the next exercise as for now, and come back to it any time later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_triangle():\n",
    "    dx = np.random.uniform(0.1, 0.3)\n",
    "    dy = np.random.uniform(0.1, 0.3)\n",
    "    noise_x = np.random.uniform(0.0, 0.1)\n",
    "    noise_y = np.random.uniform(0.0, 0.1)    \n",
    "    \n",
    "    x = np.random.uniform(0, 1-dx-noise_x)\n",
    "    y = np.random.uniform(0, 1-dy)\n",
    "    X = np.array([[x,y], [x+dx+noise_x,y], [x+dx/2, y+dy+noise_y]])\n",
    "\n",
    "    t1 = plt.Polygon(X, color='black')\n",
    "    plt.gca().add_patch(t1)\n",
    "    \n",
    "def draw_circle():\n",
    "    r = np.random.uniform(0.1, 0.25)\n",
    "    x = np.random.uniform(0+r, 1-r)\n",
    "    y = np.random.uniform(0+r, 1-r)\n",
    "\n",
    "    circle1 = plt.Circle((x, y), r, color='black')\n",
    "    plt.gcf().gca().add_artist(circle1)\n",
    "    \n",
    "def create_image(form, path):\n",
    "    plt.figure(figsize=(1, 1))\n",
    "    if form == 'circle':\n",
    "        draw_circle()\n",
    "    elif form == 'triangle':\n",
    "        draw_triangle()\n",
    "    plt.axis('off')\n",
    "    plt.savefig(path, dpi=80, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "def create_images(path):\n",
    "    nb_circles = 100\n",
    "    nb_triangles = 100\n",
    "    \n",
    "    for i in range(nb_circles):\n",
    "        c_path = os.path.join(path, 'circles', f'circle_{i}.png')\n",
    "        create_image('circle', c_path)\n",
    "        \n",
    "    for i in range(nb_triangles):\n",
    "        t_path = os.path.join(path, 'triangles', f'triangle_{i}.png')\n",
    "        create_image('triangle', t_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
