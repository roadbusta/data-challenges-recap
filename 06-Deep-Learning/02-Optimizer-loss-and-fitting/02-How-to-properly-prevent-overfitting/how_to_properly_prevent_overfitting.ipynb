{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to properly prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ **Goals of this challenge**\n",
    "- **`Cross Validate`** a Deep Learning Model\n",
    "- Give a **`Validation Set`** to the model\n",
    "- Apply two techniques to prevent overfitting:\n",
    "    - Use the **`Early Stopping`** criterion to prevent the Neural network from over-learning / over-fitting\n",
    "    - **`Regularize`** your network\n",
    "- Analyse the **`impact of the batch size and the number of epochs`** on the training of a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA MANIPULATION\n",
    "import numpy as np\n",
    "\n",
    "# DATA VISUALIZATION\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: the `blobs` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOB DATASET\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: generating a \"blobs dataset\"** ‚ùì \n",
    "\n",
    "First, let's generate some data using the [`make_blob`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) function from Scikit-Learn\n",
    "\n",
    "Generate:\n",
    "* 2000 samples with 10 features each\n",
    "* There should be 8 classes of blobs (`centers` argument), with `cluster_std` equal to 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: visualizing the dataset** ‚ùì\n",
    "\n",
    "All your samples have 10 features.\n",
    "Plot one dimension vs. another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: \"encoding\" your categorical target** ‚ùì \n",
    "\n",
    "Use the **to_categorical** function from **tensorflow.keras** to convert `y` to `y_cat` which is the categorical representation of `y` with \"*one-hot encoded*\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## (1) Cross Validation in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üë©üèª‚Äçüè´ In a previous challenge, we split the dataset into a train set and a test set at the beginning of the notebook. \n",
    "\n",
    "And then, we started to build different models which were trained on the train set and evaluated on the test set.\n",
    "\n",
    "So, at the end of the day, we used the test set everytime we evaluated our models and different hyperparameters. This is normal: we train on the train set and we evaluate on the test set.\n",
    "\n",
    "However, we selected our _\"best model\"_ based on the score of each model. In other words, we _used_ the test set to select our best model, which is a sort of ‚ùóÔ∏è `data-leakage` ‚ùóÔ∏è\n",
    "\n",
    "ü§î What should I do ? \n",
    "\n",
    "* A first good practice is to avoid using `random_state` or any deterministic separation between your train and test set. In that case, your test set will change everytime you re-run your notebook. But this is far from being sufficient.\n",
    "\n",
    "* To compare models properly, you have to run a cross-validation, a 10-fold split for instance. \n",
    "\n",
    "üßê If you look at [sklearn.model_selection.cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html), this Cross Validation tool is designed for Machine Learning algorithms implemented in Scikit Learn... whereas we have been creating and using Neural Networks from Tensorflow/Keras.\n",
    "\n",
    "üî• Let's discover how to **Cross-Validate a Neural Network**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: designing a Neural Network** ‚ùì \n",
    "\n",
    "First, write a function that generates a Neural Network with 3 layers:\n",
    "\n",
    "<u>Architecture</u>\n",
    "- an *input layer* with 25 neurons, the `relu` activation function and the appropriate `input_dim`\n",
    "- a *hidden layer* with 10 neurons and the `relu` activation function.\n",
    "- a *predictive layer* which is suited to the problem at hand (*multiclass classification*)\n",
    "\n",
    "<u>Compilation</u>\n",
    "\n",
    "The function should include a compilation method with :\n",
    "- the *categorical_crossentropy* loss, \n",
    "- the *adam* optimizer \n",
    "- the *accuracy* metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: Cross-Validating the Neural Net** ‚ùì \n",
    "\n",
    "üßëüèª‚Äçüíª Write a _loop_ using the [K-Fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) function from Scikit-Learn.\n",
    "\n",
    "* ‚úÇÔ∏è Choose 10 splits to fit your model on the train data\n",
    "* üß™ Evaluate your model on the test data. Store the results of the evaluation into a `results` variable.\n",
    "\n",
    "_Hints_:\n",
    "* ‚öñÔ∏è Do not forget to standardize your train data before fitting the neural network.\n",
    "* üëå Also, 150 epochs should be sufficient for a first approximation\n",
    "* ü§ù As this is your first Deep Learning Cross Validation, you are guided. Uncomment the following cell and follow the steps.\n",
    "* ‚è≥ Notice that we added `%% time` at the beginning of this Jupyter Notebook cell to display its running time and we encourage you to do so whenever you have expensive computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "results = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    # Split the data into train and test\n",
    "    \n",
    "    pass # YOUR CODE HERE\n",
    "    \n",
    "    # Scaling your data\n",
    "\n",
    "    pass # YOUR CODE HERE\n",
    "    \n",
    "    # Initialize the model\n",
    "\n",
    "    pass # YOUR CODE HERE    \n",
    "    \n",
    "    # Fit the model on the train data\n",
    "\n",
    "    pass # YOUR CODE HERE    \n",
    "    \n",
    "    # Evaluate the model on the test data\n",
    "    \n",
    "    pass # YOUR CODE HERE    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Performance of the Cross Validated Network** ‚ùì \n",
    "\n",
    "Print the average accuracy of these 10 folds and the standard deviation of these 10 accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó **Remarks about the computational time** ‚ùó\n",
    "\n",
    "* ü§Ø You probably encountered one of the main drawbacks of using a proper cross-validation for a Neural Network: **it takes a lot of time** ! Therefore, for the rest of the Deep-Learning module, we will do ***only one split***. \n",
    "\n",
    "* üëÆüèª‚Äç‚ôÄÔ∏è But remember that this is not entirely correct and, for real-life applications and problems, you are encouraged to use a proper cross-validation technique.\n",
    "\n",
    "* üíº In general, what practitioners do, is that they split only once, as you did. And once they get to the end of their optimization, they launch a real cross-validation at 6 PM, go home and get the final results on the next day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Holdout Method** ‚ùì \n",
    "\n",
    "For the rest of the exercise (and of the Deep Learning module), split the dataset into a train set and a test set with a 70/30% training to test data ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) How to prevent a Neural Network from overfitting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Early Stopping\n",
    "\n",
    "üö¶ ***Stop the learning process before overfitting***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: illustrating how a Neural Network can easily overfit...** ‚ùì \n",
    "\n",
    "First things first, let's show that if we train the model for too long, for too many epochs, it will overfit the training data and will not be good at predicting on the test data.\n",
    "\n",
    "To do it, train the same neural network (‚ö†Ô∏è do not forget to re-initialize it ‚ö†Ô∏è) with `validation_data =(X_test, y_test)` and `epochs = 500`. Store the history in a `history` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: your Neural Network's performance** ‚ùì \n",
    "\n",
    "Evaluate the model on the test set and print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Learnings epoch after epoch** ‚ùì \n",
    "\n",
    "Plot the history of the model with `plot_loss_accuracy` function that we coded for you.\n",
    "\n",
    "What do you observe ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history, title=None):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "    \n",
    "    # --- LOSS --- \n",
    "    \n",
    "    ax[0].plot(history.history['loss'])\n",
    "    ax[0].plot(history.history['val_loss'])\n",
    "    ax[0].set_title('Model loss')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylim((0,3))\n",
    "    ax[0].legend(['Train', 'Test'], loc='best')\n",
    "    ax[0].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[0].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    # --- ACCURACY\n",
    "    \n",
    "    ax[1].plot(history.history['accuracy'])\n",
    "    ax[1].plot(history.history['val_accuracy'])\n",
    "    ax[1].set_title('Model Accuracy')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Test'], loc='best')\n",
    "    ax[1].set_ylim((0,1))\n",
    "    ax[1].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[1].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    if title:\n",
    "        fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üë©üèª‚Äçüè´ We clearly see that ***the number of epochs we choose has a strong influence on the final results***: \n",
    "\n",
    "* **UNSUFFICIENT NUMBER OF EPOCHS $\\implies$ UNDERFITTING**:\n",
    "    * The algorithm is not optimal as its ***loss function has not converged yet***, \n",
    "    * i.e. it hasn't learned enough from the training data. \n",
    "* **TOO MANY EPOCHS** $\\implies$ **OVERFITTING**: \n",
    "    * Our neural network has ***learned too much from the training data***, even its noisy information... \n",
    "    * and the algorithm ***does not generalize well on test data***.\n",
    "\n",
    "üö¶ What we want is to basically ***stop the algorithm when the test loss is minimal*** (or when the test metrics is maximal).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "üßëüèª‚Äçüè´ Let's introduce the **`Early Stopping`** criterion.\n",
    "\n",
    "The Early Stopping criterion is a way to a***automatically stop the training of the algorithm before the end***, before the final number of epochs originally set.\n",
    "\n",
    "üïµüèª‚Äç‚ôÇÔ∏è How does it work ?\n",
    "\n",
    "> Basically, it uses part of the dataset to check whether the \"test\" loss has stopped improving. You cannot use the test data itself to check that, otherwise, it is some kind of data leakage... Instead, we will ***use a subset of the initial training data***, called the ***validation set***.\n",
    "\n",
    "\n",
    "<img src=\"validation_set.png\" alt=\"Validation set\" style=\"height:200px;\"/>\n",
    "\n",
    "‚úÇÔ∏è To split the training data, we use the **`validation_split`** keyword in  the **`.fit()`**: it sets the percentage of data from the initial training set used in the **validation set**. \n",
    "\n",
    "‚ùóÔ∏èIf you select a _validation_split_ of 30% for example, the last 30% rows of the training set will be used in the validation set. You can use **`shuffle = True`** to add some randomness.\n",
    "\n",
    "üí° On top of the _validation_split_, we use the **`callbacks`** keyword in the **`.fit()`** to call the Early Stopping criterion at the end of each epoch. You can check additional information in the [\"train_and_evaluate\" documentation for Tensorflow/Keras](https://www.tensorflow.org/guide/keras/train_and_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "‚ùì **Question: Observing the Early Stopping criterion** ‚ùì \n",
    "\n",
    "Run the following code and plot the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping()\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "# Fit the model on the train data\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split = 0.3,\n",
    "                    epochs = 500,\n",
    "                    batch_size = 16, \n",
    "                    verbose = 1, \n",
    "                    callbacks = [es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Evaluate this model that was stopped earlier** ‚ùì\n",
    "\n",
    "_(Also look at the historical losses)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó **The problem with a strict Early Stopping** ‚ùó \n",
    "\n",
    "* The problem, with this type of approach, is that as soon as the loss of the validation set increases, the model stops. \n",
    "\n",
    "* However, as a neural network's convergence is stochastic, it happens that the loss slightly increases before decreasing again. \n",
    "\n",
    "‚úÖ The `Early Stopping` criterion has a **`patience`** keyword that ***defines how many consecutive epochs without any loss decrease*** are allowed before we stop the training procedure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: EarlyStopping with patience** ‚ùì \n",
    "\n",
    "Use the Early Stopping criterion with a patience of 30 epochs, plot the results and print the accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üßëüèª‚Äçüè´ **Remarks**\n",
    "\n",
    "\n",
    "* üìâ The model continues to converge even though its loss function had some consecutive loss increases/decreases w.r.t. to the number of epochs. \n",
    "\n",
    "* ü§∑üèª‚Äç‚ôÇÔ∏è The `patience` number  to select is highly related to the task at hand and there is not any general rule of thumb. \n",
    "\n",
    "* üßëüèª‚Äçüè´ If you selected a high patience value for your Early Stopping:\n",
    "    * ü§û Your Neural Net should theoretically still stop training before the end (not always the case if your number of epochs is too low...)\n",
    "    * ‚ùóÔ∏è Your validation loss will increase again after reaching a minimum value... but ideally, you want it to be as low as possible. The validation loss potentially reaches a minimum for certain weights at a certain epoch. \n",
    "        * ü§î How to collect these weights ? \n",
    "    \n",
    "üìö The Early Stopping criterion enables you to:\n",
    "- *stop the convergence*\n",
    "- *restore the best weights of the Neural Network when it had the lowest error level (or the best score) on the validation set*, thanks to **`restore_best_weights = True`** (that is set to `False` by default).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: restore the best weights of a model** ‚ùì \n",
    "\n",
    "* Run the model with an Early Stopping criterion that will restore the best weights of the Neural Net\n",
    "* Plot the loss and accuracy \n",
    "* Print the accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü•° <u>Takeaways from the Early Stopping criteron</u>:\n",
    "\n",
    "* You can look at the üìö [**Early Stopping** documentation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) üìö to play with other parameters\n",
    "\n",
    "*  No longer need to have a look at the number of epochs as long as the model hits the stopping criterion. So, in the future, you should set a large number of epochs and the Early Stopping criterion will take care of stopping the training procedure before the model overfits! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Batch-size & Epochs\n",
    "\n",
    "üïµÔ∏è‚Äç‚ôÄÔ∏è Let's investigate the impact of the batch size on training a Neural Network and how we can use it to control how fast the parameters are updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: playing with the batch size** ‚ùì \n",
    "\n",
    "Let's run the previous model with ***different batch sizes*** (with the Early Stopping criterion included) and plot the results.\n",
    "\n",
    "üëâ Run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# RUN THIS CELL\n",
    "es = EarlyStopping(patience=20, restore_best_weights=True)\n",
    "\n",
    "for batch_size in [1, 4, 32]:\n",
    "    \n",
    "    model = initialize_model()\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_split = 0.3,\n",
    "                        epochs = 500,\n",
    "                        batch_size = batch_size, \n",
    "                        verbose = 0, \n",
    "                        callbacks = [es])\n",
    "\n",
    "    results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    plot_loss_accuracy(history, title=f'------ BATCH SIZE {batch_size} ------\\n The accuracy on the test set is of {results[1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: impact of the batch size on the convergence of a Neural Network** ‚ùì \n",
    "\n",
    "Look at the oscillations of the accuracy and the loss with respect to the batch size number. \n",
    "\n",
    "Is this coherent with what we saw when playing with the Tensorflow Playground? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Questions about the number of parameters' updates** ‚ùì \n",
    "\n",
    "* How many optimizations of the weights are done within one epoch (with respect to the number of observations and the batch size)? \n",
    "* Therefore, is one epoch longer with a large or a small batch size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Explanations</i></summary>\n",
    "\n",
    "\n",
    "***`BEGIN_EXPLANATIONS`***   \n",
    "        \n",
    "Copy-paste this code in a new cell, run it and observe.\n",
    "        \n",
    "```python\n",
    "print(f\"There are {X_train.shape[0]} rows in the training set\")\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 2\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    y_train,\n",
    "                    validation_split = 0.3,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch_size, \n",
    "                    verbose = 1)        \n",
    "```        \n",
    "\n",
    "\n",
    "<u><b><i>Number of parameters' updates:</i></b></u>\n",
    "\n",
    "* There are 1400 rows in the training set:\n",
    "    * $ 70 \\% \\times 1400 = 980 $ rows are used as the train set within the training set\n",
    "    * $ 30 \\% \\times 1400 = 420 $ rows are used as the validation set  \n",
    "\n",
    "* For each epoch, we run a forward/backward propagation $ \\large \\lceil \\frac{980}{batch size} \\rceil = \\lceil \\frac{980}{16}\\rceil = 62$ times\n",
    "\n",
    "* Without the Early Stopping Criterion, we will have overall _number of_ $ epochs \\times 62 = 2 \\times 62 = 124 $ iterations \n",
    "        \n",
    "<u><b><i>Impact of the batch size:</i></b></u>\n",
    "\n",
    "The smaller the batch size \n",
    "\n",
    "$ \\implies $ The more sub-iterations will be done \n",
    "\n",
    "$ \\implies $ Parameters will be updated more frequently \n",
    "\n",
    "$ \\implies $ We may need less epochs \n",
    "\n",
    "_(but we also don't care so much about the number of epochs if we use an Early Stopping Criterion with patience anyway)_    \n",
    "\n",
    "***`END_EXPLANATIONS`***    \n",
    "    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 2\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    y_train,\n",
    "                    validation_split = 0.3,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch_size, \n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm455OX6ksyl"
   },
   "source": [
    "‚ùóÔ∏è ***Neural Networks with a complex architecture  can quickly lead to overfitting*** (_\"too many\" layers and/or neurons..._). ‚ùóÔ∏è\n",
    "\n",
    "üî• But as Dense Neural Networks are _in fine_ activated linear regressions, the weights can be constrained using L1, L2 or L1-L2 penalties!\n",
    "\n",
    "\n",
    "üöì So, let's apply some L2 or L1 penalties to these neurons. As in Machine Learning, these penalties are called ***`Regularizers`***.\n",
    "\n",
    "\n",
    "\n",
    "üìö <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/regularizers\">**tensorflow/keras/regularizers**</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Initial Question: observing overfitting one more time before regularizing...**‚ùì\n",
    " \n",
    "* First, let's initialize a model that has too many parameters for the task such that it overfits the training data quickly. _To that purpose, let's not use any Early Stopping criterion._\n",
    "\n",
    "üéÅ Let's not waste time re-coding something that you are now used to, just run the following cell and observe what is happening üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 81596,
     "status": "ok",
     "timestamp": 1612905145614,
     "user": {
      "displayName": "Bruno Lajoie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg0dl6gThG8gOPbCvHbgt62zQnsi8cgbQ7C5HkD_Cg=s64",
      "userId": "15793030209206844069"
     },
     "user_tz": -60
    },
    "id": "XaOTe0-Yksyn"
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "\n",
    "# 1. Model Architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(25, activation='relu', input_dim=10))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='softmax'))\n",
    "\n",
    "# 2. Model compilation\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 3. Training \n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split = 0.3,\n",
    "                    epochs = 300,           # Notice that we are not using any Early Stopping Criterion\n",
    "                    batch_size = 16, \n",
    "                    verbose=0)\n",
    "\n",
    "# 4. Evaluation\n",
    "results_train = model.evaluate(X_train, y_train, verbose = 0)\n",
    "results_test = model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "\n",
    "# 5. Looking back at what happened during the training phase\n",
    "print(f'The accuracy on the test set is {results_test[1]:.2f}...')\n",
    "print(f'...whereas the accuracy on the training set is {results_train[1]:.2f}!')\n",
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è In our *over-parametrized network*, ***some neurons became too specific to the given training data***, preventing the network from generalizing to new data. \n",
    "\n",
    "üòï This led to some overfitting...! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: studying the impact of regularization on the Neural Net**‚ùì\n",
    " \n",
    "* Change the previous code to integrate a L2 or L1 regularizer to your different Dense Layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.4) Dropout layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÇÔ∏è **`Dropout Layers`** ‚úÇÔ∏è\n",
    "\n",
    "- ‚úÖ They are super easy/straightforward to code \n",
    "- üë©üèª‚Äçüè´ But what is going on _\"under the hood\"_ ? (You love this expression right ? üòè)\n",
    "\n",
    "<u><i>Introduction to Dropout Layers</i><u>\n",
    "\n",
    "* Their role is to randomly cancel the output of some neurons during the training phase\n",
    "* It prevents the network from getting too specific to the input data. With Dropout layers,\n",
    "    - No neuron can overspecialize in learning a pattern of the dataset because its output sometimes canceled by the Dropout Layer\n",
    "    - Overall, a Dropout Layer forces the information of an input to go through multiple neurons instead of only one specific neuron\n",
    "\n",
    "<details>\n",
    "    <summary><i>[optional] Untoggle this for further explanations about Dropout Layers</i></summary>\n",
    "\n",
    "ü§î Remember what we said about ***neurons becoming too specific*** ? \n",
    "\n",
    "---\n",
    "\n",
    "‚öΩÔ∏è *An analogy with football:* \n",
    "\n",
    "Think about a football team. There are 11 players on the pitch, but there are also players on the bench, what for ? \n",
    "* The coach may want to substitute injured or tired players during the game\n",
    "* The team may be:\n",
    "    - losing the game and willing to replace a midfielder with a more ***versatile player*** who can be more offensive (= ***mix of*** midfielder/attacker role) \n",
    "    - winning the game and would like to secure the result by replacing a midfielder with a more ***versatile player*** who can be more defense ( = ***mix of*** midfielder/defense role)\n",
    "    \n",
    "üí° Well, in a Neural Net, there is also a risk that a neuron over-specialises in detecting one pattern, whereas it would be more advisable to keep it more versatile.\n",
    "    \n",
    "---\n",
    "\n",
    "üìö **The goal of  [Dropout layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) is to prevent neurons from over-specializing**.\n",
    "\n",
    "üßëüèª‚Äçüè´ How do Dropout layers work ? \n",
    "\n",
    "* üëâ When we apply a **Dropout rate** of 20% to a layer $k$, 20% of ***selected neurons*** will have their ***weights temporarily set to 0*** , which has the following consequences:\n",
    "    1. A neuron computed at layer $k+1$ considers all the weights as usual, but since some of them were temporarily set to 0, they were ‚Äúignored‚Äù while creating the neurons of this layer $k+1$\n",
    "    2. Once a batch of data points have been seen by the Neural Network (**forward propagation**) and 80% of the weights in a layer optimized/updated (**backward propagation** with the **adam** optimizer), the Neural Network moves on to the forward propagation of the next batch, where it will use :\n",
    "        * 80% of the weights, those which were updated\n",
    "        * 20% of weights, those which were previously ignored (which kept their original values and were actually not set in stone to 0 but just temporarily)\n",
    "        * Based on these weights, we will again go through this Dropout Layer which is going to ignore again 20% of the weights of a given layer temporarily. Be careful, the selection of these 20% being random, the weights which will be ignored this time won't necessarily be the same weights as before !\n",
    "\n",
    "* üëâ Think about the ***Dropout Rate*** as a ***sampling with replacement***\n",
    "\n",
    "* üëâ By ***randomly shutting down*** different neurons at different epochs, we ***force the datapoints*** to be analysed by different neurons. At the end of the day, these neurons will become ***more versatile*** instead of overspecializing!\n",
    "    \n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: studying the impact of Dropout layers on the Neural Net**‚ùì\n",
    "\n",
    "* Include some Dropout layers in the architecture of your original model. \n",
    "_(Don't add regularizers here)_\n",
    "\n",
    "* What can you observe on the convergence of your model ? What about its power of generalization ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b>Conclusions:</b></u>\n",
    "\n",
    "* ‚ùóÔ∏è If you train a Neural Network too much (too many layers, too many neurons and/or too many epochs), it will overfit very quickly\n",
    "\n",
    "* ü•ä To prevent overfitting in Deep Learning, you can use Early Stopping, Regularization and Dropout Layers.\n",
    "\n",
    "* ‚ùóÔ∏è Be careful, if you use these techniques, you might end up on the other spectrum of the performance where you model would underfit the dataset!\n",
    "\n",
    "* üÜó You could potentially look at the batch size but we usually use 16 or 32. And as the French Computer Scientist [**Yann Lecun**](https://en.wikipedia.org/wiki/Yann_LeCun) said (Facebook x NYU):\n",
    "\n",
    "<img src=\"https://github.com/lewagon/data-images/blob/master/DL/deep_learning_yann_lecun_batch_size.png?raw=true\" alt=\"batch_size_yann_lecun\" width=\"400\" height=\"250\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations !\n",
    "\n",
    "üíæ Do not forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move to the next challenge !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
